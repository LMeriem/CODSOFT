# -*- coding: utf-8 -*-
"""CREDIT CARD FRAUD DETECTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cfOTBuzxS_PjcJzzd9Xlfg20Rl71uDJE
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, make_scorer, fbeta_score
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier

# Load the dataset
data = pd.read_csv('fraudTrain.csv')

# Display the first few rows of the dataset and the column names
data.head(10)

# Clean the column names by stripping whitespace
data.columns = data.columns.str.strip()

# Data preprocessing
data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'], format='%Y-%m-%d %H:%M:%S')
data['trans_year'] = data['trans_date_trans_time'].dt.year
data['trans_month'] = data['trans_date_trans_time'].dt.month
data['trans_day'] = data['trans_date_trans_time'].dt.day
data['trans_hour'] = data['trans_date_trans_time'].dt.hour
data['trans_minute'] = data['trans_date_trans_time'].dt.minute
data['trans_second'] = data['trans_date_trans_time'].dt.second

# Display the current columns after feature extraction
print("Current columns after feature extraction:", data.columns.tolist())

# Drop unnecessary columns
data.drop(['trans_num', 'unix_time', 'first', 'last', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'dob', 'trans_date_trans_time'], axis=1, inplace=True)

# Convert categorical variables to dummy/indicator variables
data = pd.get_dummies(data, columns=['merchant', 'category', 'gender', 'job'], drop_first=True)

# Features and target variable
X = data.drop('is_fraud', axis=1)  # Features
y = data['is_fraud']  # Target variable

# Handle missing values in y
if y.isnull().any():
    print("Missing values found in target variable 'y'. Dropping these rows.")
    # Drop rows with missing target values
    valid_indices = y.notnull()
    X = X[valid_indices]
    y = y[valid_indices]

# Handle missing values
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)

# Apply SMOTE to balance classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression
logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train, y_train)
y_pred_logistic = logistic_model.predict(X_test)

# Decision Tree
tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)
y_pred_tree = tree_model.predict(X_test)

# Define a custom F2 scorer
f2_scorer = make_scorer(fbeta_score, beta=2)

# Random Forest with Hyperparameter Tuning
rf_params = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Use the custom F2 scorer in GridSearchCV
rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, scoring=f2_scorer, cv=3, n_jobs=-1)
rf_grid_search.fit(X_train, y_train)

# Get the best Random Forest model
best_rf_model = rf_grid_search.best_estimator_
y_pred_forest = best_rf_model.predict(X_test)

# Evaluate and print classification reports
print("Logistic Regression:")
print(classification_report(y_test, y_pred_logistic))

print("Decision Tree:")
print(classification_report(y_test, y_pred_tree))

print("Random Forest (Best Model):")
print(classification_report(y_test, y_pred_forest))